{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [PYTORCH: TENSORS AND AUTOGRAD](https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_autograd.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ÂÖ®ËøûÊé•ReLUÁΩëÁªúÔºö1Â±ÇÈöêËóèÂ±Ç„ÄÅÊó†ÂÅèÁΩÆÔºåÊ†πÊçÆùë•È¢ÑÊµãùë¶ÔºåÈÄöËøáÊúÄÂ∞èÂåñÊ¨ßÊ∞èË∑ùÁ¶ªËÆ≠ÁªÉÁΩëÁªú„ÄÇ\n",
    "\n",
    "pytorchÂº†ÈáèËÆ°ÁÆóÁΩëÁªúÂâçÂêë‰º†Êí≠„ÄÅÊçüÂ§±ÔºåautogradËÆ°ÁÆóÊ¢ØÂ∫¶„ÄÇ\n",
    "\n",
    "```python\n",
    "x.requires_grad=True\n",
    "x.grad\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 32181034.0\n",
      "1 27363428.0\n",
      "2 25042764.0\n",
      "3 21873754.0\n",
      "4 17196638.0\n",
      "5 12014108.0\n",
      "6 7707678.5\n",
      "7 4772950.0\n",
      "8 3009209.25\n",
      "9 1999151.875\n",
      "10 1419613.75\n",
      "11 1071914.75\n",
      "12 849309.5\n",
      "13 696212.0625\n",
      "14 583994.1875\n",
      "15 497719.0\n",
      "16 428769.3125\n",
      "17 372139.40625\n",
      "18 324883.28125\n",
      "19 284970.59375\n",
      "20 250938.5625\n",
      "21 221735.75\n",
      "22 196567.921875\n",
      "23 174822.421875\n",
      "24 155916.09375\n",
      "25 139384.84375\n",
      "26 124917.2578125\n",
      "27 112186.9453125\n",
      "28 100962.125\n",
      "29 91031.1796875\n",
      "30 82216.4140625\n",
      "31 74383.7578125\n",
      "32 67401.6640625\n",
      "33 61165.66796875\n",
      "34 55583.45703125\n",
      "35 50586.6328125\n",
      "36 46103.5859375\n",
      "37 42069.46484375\n",
      "38 38437.16015625\n",
      "39 35159.73828125\n",
      "40 32198.56640625\n",
      "41 29516.841796875\n",
      "42 27093.1015625\n",
      "43 24892.05078125\n",
      "44 22892.142578125\n",
      "45 21072.3984375\n",
      "46 19414.25\n",
      "47 17900.259765625\n",
      "48 16517.447265625\n",
      "49 15252.9853515625\n",
      "50 14095.67578125\n",
      "51 13035.2587890625\n",
      "52 12062.8173828125\n",
      "53 11170.2763671875\n",
      "54 10350.642578125\n",
      "55 9597.4619140625\n",
      "56 8904.47265625\n",
      "57 8266.1171875\n",
      "58 7677.85888671875\n",
      "59 7135.7431640625\n",
      "60 6635.23291015625\n",
      "61 6172.96875\n",
      "62 5745.8017578125\n",
      "63 5350.869140625\n",
      "64 4985.5830078125\n",
      "65 4647.3369140625\n",
      "66 4334.12646484375\n",
      "67 4043.59716796875\n",
      "68 3774.208740234375\n",
      "69 3524.304931640625\n",
      "70 3292.407958984375\n",
      "71 3076.979248046875\n",
      "72 2876.733154296875\n",
      "73 2690.606689453125\n",
      "74 2517.3984375\n",
      "75 2356.233154296875\n",
      "76 2206.097900390625\n",
      "77 2066.220947265625\n",
      "78 1935.887451171875\n",
      "79 1814.3555908203125\n",
      "80 1700.9921875\n",
      "81 1595.2130126953125\n",
      "82 1496.461669921875\n",
      "83 1404.2423095703125\n",
      "84 1318.055419921875\n",
      "85 1237.521484375\n",
      "86 1162.2255859375\n",
      "87 1091.7921142578125\n",
      "88 1025.8883056640625\n",
      "89 964.201171875\n",
      "90 906.4501953125\n",
      "91 852.3665161132812\n",
      "92 801.6898193359375\n",
      "93 754.2120971679688\n",
      "94 709.7034912109375\n",
      "95 667.961669921875\n",
      "96 628.814453125\n",
      "97 592.0835571289062\n",
      "98 557.6195068359375\n",
      "99 525.2628784179688\n",
      "100 494.8809814453125\n",
      "101 466.3531799316406\n",
      "102 439.54913330078125\n",
      "103 414.36407470703125\n",
      "104 390.68853759765625\n",
      "105 368.43072509765625\n",
      "106 347.50433349609375\n",
      "107 327.8200988769531\n",
      "108 309.3067321777344\n",
      "109 291.880859375\n",
      "110 275.4862060546875\n",
      "111 260.0487365722656\n",
      "112 245.51190185546875\n",
      "113 231.82345581054688\n",
      "114 218.92909240722656\n",
      "115 206.7820587158203\n",
      "116 195.34036254882812\n",
      "117 184.5562744140625\n",
      "118 174.38833618164062\n",
      "119 164.80648803710938\n",
      "120 155.7703094482422\n",
      "121 147.24661254882812\n",
      "122 139.2073211669922\n",
      "123 131.6237335205078\n",
      "124 124.46783447265625\n",
      "125 117.7143325805664\n",
      "126 111.34249877929688\n",
      "127 105.32764434814453\n",
      "128 99.64695739746094\n",
      "129 94.28423309326172\n",
      "130 89.21978759765625\n",
      "131 84.43584442138672\n",
      "132 79.91621398925781\n",
      "133 75.64677429199219\n",
      "134 71.61344146728516\n",
      "135 67.80152893066406\n",
      "136 64.19906616210938\n",
      "137 60.793052673339844\n",
      "138 57.573238372802734\n",
      "139 54.53020095825195\n",
      "140 51.651527404785156\n",
      "141 48.92886734008789\n",
      "142 46.35389709472656\n",
      "143 43.91883850097656\n",
      "144 41.61512756347656\n",
      "145 39.4344596862793\n",
      "146 37.37199401855469\n",
      "147 35.41984558105469\n",
      "148 33.57217788696289\n",
      "149 31.823516845703125\n",
      "150 30.16897201538086\n",
      "151 28.602027893066406\n",
      "152 27.118566513061523\n",
      "153 25.71350860595703\n",
      "154 24.382827758789062\n",
      "155 23.123401641845703\n",
      "156 21.9300537109375\n",
      "157 20.800397872924805\n",
      "158 19.729694366455078\n",
      "159 18.71529197692871\n",
      "160 17.753952026367188\n",
      "161 16.843387603759766\n",
      "162 15.980587005615234\n",
      "163 15.162763595581055\n",
      "164 14.38802433013916\n",
      "165 13.653918266296387\n",
      "166 12.957239151000977\n",
      "167 12.297653198242188\n",
      "168 11.670952796936035\n",
      "169 11.077716827392578\n",
      "170 10.515484809875488\n",
      "171 9.981866836547852\n",
      "172 9.47633171081543\n",
      "173 8.996761322021484\n",
      "174 8.541831016540527\n",
      "175 8.109617233276367\n",
      "176 7.700655937194824\n",
      "177 7.312201023101807\n",
      "178 6.943570613861084\n",
      "179 6.594073295593262\n",
      "180 6.262358665466309\n",
      "181 5.947750568389893\n",
      "182 5.649077415466309\n",
      "183 5.365675449371338\n",
      "184 5.09650182723999\n",
      "185 4.841512680053711\n",
      "186 4.599235534667969\n",
      "187 4.369096755981445\n",
      "188 4.151308059692383\n",
      "189 3.943650960922241\n",
      "190 3.7469987869262695\n",
      "191 3.5603229999542236\n",
      "192 3.3830130100250244\n",
      "193 3.214691400527954\n",
      "194 3.054745674133301\n",
      "195 2.9029977321624756\n",
      "196 2.758903741836548\n",
      "197 2.6219370365142822\n",
      "198 2.491938352584839\n",
      "199 2.3682804107666016\n",
      "200 2.251215934753418\n",
      "201 2.1399028301239014\n",
      "202 2.0341150760650635\n",
      "203 1.9335336685180664\n",
      "204 1.8381333351135254\n",
      "205 1.7473193407058716\n",
      "206 1.6612634658813477\n",
      "207 1.579493522644043\n",
      "208 1.5016447305679321\n",
      "209 1.427748441696167\n",
      "210 1.3574061393737793\n",
      "211 1.2907118797302246\n",
      "212 1.2272498607635498\n",
      "213 1.1670355796813965\n",
      "214 1.109915852546692\n",
      "215 1.055266261100769\n",
      "216 1.0037285089492798\n",
      "217 0.9544355273246765\n",
      "218 0.9077669978141785\n",
      "219 0.8633190989494324\n",
      "220 0.8211161494255066\n",
      "221 0.780935525894165\n",
      "222 0.7427470088005066\n",
      "223 0.7065861821174622\n",
      "224 0.6720170378684998\n",
      "225 0.6392611861228943\n",
      "226 0.6081122756004333\n",
      "227 0.5784304738044739\n",
      "228 0.550309419631958\n",
      "229 0.5233911871910095\n",
      "230 0.49794113636016846\n",
      "231 0.47368815541267395\n",
      "232 0.4507182538509369\n",
      "233 0.4288589358329773\n",
      "234 0.4080356955528259\n",
      "235 0.38817599415779114\n",
      "236 0.36934950947761536\n",
      "237 0.3514576852321625\n",
      "238 0.3344000279903412\n",
      "239 0.3182400166988373\n",
      "240 0.302862286567688\n",
      "241 0.2880939841270447\n",
      "242 0.2742033004760742\n",
      "243 0.26095178723335266\n",
      "244 0.24833248555660248\n",
      "245 0.23630768060684204\n",
      "246 0.2249155044555664\n",
      "247 0.2140195667743683\n",
      "248 0.20371967554092407\n",
      "249 0.19391079246997833\n",
      "250 0.18454807996749878\n",
      "251 0.17564328014850616\n",
      "252 0.16719190776348114\n",
      "253 0.15916088223457336\n",
      "254 0.151503786444664\n",
      "255 0.14419086277484894\n",
      "256 0.13720327615737915\n",
      "257 0.13061925768852234\n",
      "258 0.12431632727384567\n",
      "259 0.1183648630976677\n",
      "260 0.11267373710870743\n",
      "261 0.10730483382940292\n",
      "262 0.10212620347738266\n",
      "263 0.09720321744680405\n",
      "264 0.09254702925682068\n",
      "265 0.08813005685806274\n",
      "266 0.08390854299068451\n",
      "267 0.07986168563365936\n",
      "268 0.07604334503412247\n",
      "269 0.07238972187042236\n",
      "270 0.06893530488014221\n",
      "271 0.06564168632030487\n",
      "272 0.06248463690280914\n",
      "273 0.05950254574418068\n",
      "274 0.05664604529738426\n",
      "275 0.0539521723985672\n",
      "276 0.05137370526790619\n",
      "277 0.04891730472445488\n",
      "278 0.046601343899965286\n",
      "279 0.04437605291604996\n",
      "280 0.0422733910381794\n",
      "281 0.04025926813483238\n",
      "282 0.038338836282491684\n",
      "283 0.036512091755867004\n",
      "284 0.034766778349876404\n",
      "285 0.03313151001930237\n",
      "286 0.03153903782367706\n",
      "287 0.030064033344388008\n",
      "288 0.028636831790208817\n",
      "289 0.027281709015369415\n",
      "290 0.025984549894928932\n",
      "291 0.024755652993917465\n",
      "292 0.023594969883561134\n",
      "293 0.02249114401638508\n",
      "294 0.021426014602184296\n",
      "295 0.02041596919298172\n",
      "296 0.019445210695266724\n",
      "297 0.018527233973145485\n",
      "298 0.01766102761030197\n",
      "299 0.016837699338793755\n",
      "300 0.016042286530137062\n",
      "301 0.015295694582164288\n",
      "302 0.014583678916096687\n",
      "303 0.013905234634876251\n",
      "304 0.013253760524094105\n",
      "305 0.012639124877750874\n",
      "306 0.012048998847603798\n",
      "307 0.01148225273936987\n",
      "308 0.010957765392959118\n",
      "309 0.010450218804180622\n",
      "310 0.009963005781173706\n",
      "311 0.009503982961177826\n",
      "312 0.009062733501195908\n",
      "313 0.008646423928439617\n",
      "314 0.008251356892287731\n",
      "315 0.007881381548941135\n",
      "316 0.007517178542912006\n",
      "317 0.007174822501838207\n",
      "318 0.006842550355941057\n",
      "319 0.006532703526318073\n",
      "320 0.006238347850739956\n",
      "321 0.00595659576356411\n",
      "322 0.005688899662345648\n",
      "323 0.005437037907540798\n",
      "324 0.0051903980784118176\n",
      "325 0.004962925333529711\n",
      "326 0.004743610508739948\n",
      "327 0.004528580699115992\n",
      "328 0.004334091674536467\n",
      "329 0.0041445717215538025\n",
      "330 0.003961783368140459\n",
      "331 0.0037860427983105183\n",
      "332 0.003622842952609062\n",
      "333 0.0034664147533476353\n",
      "334 0.0033156219869852066\n",
      "335 0.0031736185774207115\n",
      "336 0.0030390331521630287\n",
      "337 0.0029111881740391254\n",
      "338 0.0027876633685082197\n",
      "339 0.0026709679514169693\n",
      "340 0.0025569023564457893\n",
      "341 0.002453390508890152\n",
      "342 0.0023505648132413626\n",
      "343 0.002250280464068055\n",
      "344 0.00215957616455853\n",
      "345 0.0020705407951027155\n",
      "346 0.001986422576010227\n",
      "347 0.0019054478034377098\n",
      "348 0.001830971334129572\n",
      "349 0.0017566424794495106\n",
      "350 0.0016885767690837383\n",
      "351 0.001621009549126029\n",
      "352 0.0015577970771118999\n",
      "353 0.0014976240927353501\n",
      "354 0.0014394207391887903\n",
      "355 0.0013826389331370592\n",
      "356 0.0013295263051986694\n",
      "357 0.001279227901250124\n",
      "358 0.0012314020423218608\n",
      "359 0.0011851036688312888\n",
      "360 0.0011423317482694983\n",
      "361 0.0011001426028087735\n",
      "362 0.0010591919999569654\n",
      "363 0.0010202398989349604\n",
      "364 0.0009840634884312749\n",
      "365 0.0009483597823418677\n",
      "366 0.0009130611433647573\n",
      "367 0.0008807645062915981\n",
      "368 0.000850546988658607\n",
      "369 0.000819308974314481\n",
      "370 0.0007914237212389708\n",
      "371 0.000764385680668056\n",
      "372 0.0007384339114651084\n",
      "373 0.0007134677725844085\n",
      "374 0.0006908149225637317\n",
      "375 0.0006663815001957119\n",
      "376 0.000645721796900034\n",
      "377 0.0006249132566154003\n",
      "378 0.0006048846407793462\n",
      "379 0.0005849619046784937\n",
      "380 0.0005665404605679214\n",
      "381 0.0005485808360390365\n",
      "382 0.0005299581680446863\n",
      "383 0.0005146960611455142\n",
      "384 0.0004991103196516633\n",
      "385 0.0004836571461055428\n",
      "386 0.0004688063927460462\n",
      "387 0.00045402374234981835\n",
      "388 0.0004408999520819634\n",
      "389 0.00042747060069814324\n",
      "390 0.00041548581793904305\n",
      "391 0.0004035091551486403\n",
      "392 0.00039141500019468367\n",
      "393 0.00037929334212094545\n",
      "394 0.00036836188519373536\n",
      "395 0.000358217308530584\n",
      "396 0.00034785267780534923\n",
      "397 0.0003381675051059574\n",
      "398 0.00032875320175662637\n",
      "399 0.0003188863047398627\n",
      "400 0.0003108512028120458\n",
      "401 0.000302022323012352\n",
      "402 0.00029419531347230077\n",
      "403 0.00028581207152456045\n",
      "404 0.0002781492657959461\n",
      "405 0.00027142747421748936\n",
      "406 0.0002639097219798714\n",
      "407 0.0002570843789726496\n",
      "408 0.00024997000582516193\n",
      "409 0.00024375894281547517\n",
      "410 0.00023751784465275705\n",
      "411 0.00023133878130465746\n",
      "412 0.00022525111853610724\n",
      "413 0.00021960695448797196\n",
      "414 0.00021375584765337408\n",
      "415 0.00020913318439852446\n",
      "416 0.0002031805197475478\n",
      "417 0.00019832863472402096\n",
      "418 0.00019329834321979433\n",
      "419 0.0001887940743472427\n",
      "420 0.00018469734641257674\n",
      "421 0.00018025055760517716\n",
      "422 0.00017569372721482068\n",
      "423 0.00017189758364111185\n",
      "424 0.00016787835920695215\n",
      "425 0.00016381696332246065\n",
      "426 0.00016008055536076427\n",
      "427 0.00015636814350727946\n",
      "428 0.00015318546502385288\n",
      "429 0.00014954204380046576\n",
      "430 0.00014615936379414052\n",
      "431 0.00014276908768806607\n",
      "432 0.00013961471267975867\n",
      "433 0.00013667238818015903\n",
      "434 0.00013356516137719154\n",
      "435 0.0001305771293118596\n",
      "436 0.00012810704356525093\n",
      "437 0.0001252990768989548\n",
      "438 0.0001224367442773655\n",
      "439 0.00011999312118859962\n",
      "440 0.00011741765774786472\n",
      "441 0.00011524008004926145\n",
      "442 0.00011282024206593633\n",
      "443 0.00011063929559895769\n",
      "444 0.00010835588909685612\n",
      "445 0.00010606781870592386\n",
      "446 0.00010424781794426963\n",
      "447 0.00010207181912846863\n",
      "448 9.997589950216934e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449 9.799198596738279e-05\n",
      "450 9.599179611541331e-05\n",
      "451 9.423636947758496e-05\n",
      "452 9.257062629330903e-05\n",
      "453 9.087519720196724e-05\n",
      "454 8.944678120315075e-05\n",
      "455 8.742039790377021e-05\n",
      "456 8.553919906262308e-05\n",
      "457 8.403001993428916e-05\n",
      "458 8.250909741036594e-05\n",
      "459 8.131912181852385e-05\n",
      "460 7.963482494233176e-05\n",
      "461 7.839352474547923e-05\n",
      "462 7.721388101344928e-05\n",
      "463 7.588497828692198e-05\n",
      "464 7.465945236617699e-05\n",
      "465 7.340106094488874e-05\n",
      "466 7.218454265967011e-05\n",
      "467 7.111355080269277e-05\n",
      "468 6.999125616857782e-05\n",
      "469 6.856793334009126e-05\n",
      "470 6.758483505109325e-05\n",
      "471 6.638457853114232e-05\n",
      "472 6.567416130565107e-05\n",
      "473 6.449961801990867e-05\n",
      "474 6.331574695650488e-05\n",
      "475 6.248529098229483e-05\n",
      "476 6.157668394735083e-05\n",
      "477 6.0308688262011856e-05\n",
      "478 5.9341378801036626e-05\n",
      "479 5.879437958355993e-05\n",
      "480 5.760627027484588e-05\n",
      "481 5.6728724302956834e-05\n",
      "482 5.59602485736832e-05\n",
      "483 5.496264566318132e-05\n",
      "484 5.413146209320985e-05\n",
      "485 5.327472899807617e-05\n",
      "486 5.260075340629555e-05\n",
      "487 5.1901773986173794e-05\n",
      "488 5.129879355081357e-05\n",
      "489 5.059209433966316e-05\n",
      "490 5.002614125260152e-05\n",
      "491 4.92630060762167e-05\n",
      "492 4.8645601054886356e-05\n",
      "493 4.806627475772984e-05\n",
      "494 4.720835204352625e-05\n",
      "495 4.6500368625856936e-05\n",
      "496 4.586150316754356e-05\n",
      "497 4.513450403464958e-05\n",
      "498 4.451855056686327e-05\n",
      "499 4.413289570948109e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Tensors during the backward pass.\n",
    "x = torch.randn(*(N, D_in), device=device, dtype=dtype)\n",
    "y = torch.randn(*(N, D_out), device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y using operations on Tensors; these\n",
    "    # are exactly the same operations we used to compute the forward pass using\n",
    "    # Tensors, but we do not need to keep references to intermediate values since\n",
    "    # we are not implementing the backward pass by hand.\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the a scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "    \n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "    \n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    # An alternative way is to operate on weight.data and weight.grad.data.\n",
    "    # Recall that tensor.data gives a tensor that shares the storage with\n",
    "    # tensor, but doesn't track history.\n",
    "    # You can also use torch.optim.SGD to achieve this.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
